---
title: "HW8"
author: "Irene Yang"
date: "4/18/2018"
output:
  word_document: default
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

K-nearest neighbor

Let's try a variation on the NHANES data set again.

```{r}
library(tidyverse)
library(class)
library(rpart)
library(NHANES)
library(RColorBrewer)
library(plot3D)
library(parallel)
library(randomForestSRC)
library(ggRandomForests)
library(mosaic)

# Create the NHANES dataset again
```

Create the NHANES dataset again, just like we did in class, only using sleep trouble (variable name = SleepTrouble) as the dependent variable, instead of SleepTrouble. 

```{r}

# Create the NHANES dataset again

people2 <- NHANES %>% dplyr::select(Age, Gender, SleepTrouble, BMI, HHIncome, PhysActive) 
#%>% na.omit()

glimpse(people2)

```

####Problem 1

What is the marginal distribution of sleep trouble?

```{r Problem 1}

tally(~ SleepTrouble, data = people2, format = "percent")

```

Recall from our prior work, the packages work better if the dataset is a dataframe, and the variables are numeric.

```{r}

class(people2)

# Convert back to dataframe
people2 <- as.data.frame(people2)
glimpse(people2)

# Convert factors to numeric - the packages just seem to work better that way
people2$Gender <- as.numeric(people2$Gender)
people2$SleepTrouble <- as.numeric(people2$SleepTrouble)
people2$HHIncome <- as.numeric(people2$HHIncome)
people2$PhysActive <- as.numeric(people2$PhysActive)

people2 <- na.omit(people2)

glimpse(people2)

```

####Problem 2

Apply the k-nearest neighbor procedure to predict SleepTrouble from the other covariates, as we did for SleepTrouble. Use k = 1, 3, 5, and 20.

```{r}

#Apply k-nearest neighbor approach to predict SleepTrouble for k = 1, 3, 5, 20

# Let's try different values of k to see how that affects performance. This is taking different numbers of nearest neighbors
knn.1 <- knn(train = people2, test = people2, cl = as.numeric(people2$SleepTrouble), k = 1)
knn.3 <- knn(train = people2, test = people2, cl = people2$SleepTrouble, k = 3)
knn.5 <- knn(train = people2, test = people2, cl = people2$SleepTrouble, k = 5)
knn.20 <- knn(train = people2, test = people2, cl = people2$SleepTrouble, k = 20)

#knn.1
```

####Problem 3

Now let's see how well these classifiers work overall

```{r}

# How well do these classifiers (k = 1, 3, 5, 20) work? Calculate the percent predicted correctly

100*sum(people2$SleepTrouble == knn.1)/length(knn.1)
100*sum(people2$SleepTrouble == knn.3)/length(knn.3)
100*sum(people2$SleepTrouble == knn.5)/length(knn.5)
100*sum(people2$SleepTrouble == knn.20)/length(knn.20)

```

Similar to our in class exercise with diabetes, we see that as k increases, the prediction worsens, but this is expected. Prediction does seem to be poorer for SleepTrouble compared to Diabetes.

####Problem 4

What about success overall?

```{r}

# Another way to look at success rate against increasing k

table(knn.1, people2$SleepTrouble)
table(knn.3, people2$SleepTrouble)
table(knn.5, people2$SleepTrouble)
table(knn.20, people2$SleepTrouble)

```

This confirms what we saw earlier.  k=1 perfectly predicts SleepTrouble and prediction worsens as k increases.

Github respository:  https://github.com/iyang5/Homework-8.git

